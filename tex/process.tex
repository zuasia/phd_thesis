%!TEX root=../paper.tex

\chapter{Managing Enhanced Performance Variation on Adaptive Clocking Multicore Processors}
\label{sec:process}

In this chapter, we discuss how to leverage active timing margin's automatic timing margin tracking ability to expose a multicore's static core-to-core performance heterogeneity caused by process variation, and explore how to manage the dynamically occurring inter-core frequency interference caused by the cores sharing the power delivery on an active timing margin system.

On multi-core and many-core chips, it is critical that we push down timing margin that not only deals with the dynamically occurring effects such as temperature and voltage variation but also covers the core-to-core performance heterogeneity caused by lithography's manufacturing process variation. To investigate this issue, we fine-tune the hardware active timing margin solution designed to cope with voltage noise, such as the adaptive clocking fabric in the POWER7+ multicore in~\Sec{sec:voltage}. We study enhancing a multicore's active timing margin capability according to each core's characteristics, as well as the running applications' characteristics. Adaptive clocking's per-core configurable control loop provides a new opportunity to expose the inter-core speed variation and to provide more performance gain than the conventional multicore process variation, i.e., calibrating static frequency levels separately for each core~\cite{sarangi2008varius,teodorescu2008variation,rangan2009thread,dighe2010within,rangan2011achieving}.

The conventional approach to expose core-to-core variation uses per-core $\textless$$v,f$$\textgreater$ setpoint with static margins and thus requires guarding against worst-case voltage variation, such as the $di/dt$ effect and the DC voltage drop across the chip's power delivery path, each of which can consume 3\% of the $V_{dd}$~\cite{zu2015adaptive}. But because active timing margin can handle these adaptively, it provides more performance gain by exploiting the inherent inter-core variation in the processor.

\begin{figure}[t]
  \centering
  \includegraphics[trim=0 40 0 10,clip,width=.75\linewidth]{graphs/process/schedule-space.pdf}
  \caption{Fine-tuning active timing margin (ATM) exposes both process (P) and voltage (V) variation, and improves frequency compared with the default active timing margin configuration and the per-core $\textless$$v,f$$\textgreater$ static margin setpoints.}

  \label{fig:motivate-variation}
\end{figure}

\Fig{fig:motivate-variation} illustrates the performance enhancement and heterogeneity exposed by ``fine-tuning'' the adaptive clocking control loop for each core. On the tested POWER7+ platform, we (re)configure active timing margin via its Critical Path Monitors (CPMs). The CPM is the chip's programmable canary circuit that measures the timing margin~\cite{lefurgy2011active, drake2013single}.Similar interfaces exist on other adaptive clocking systems for test-time calibration of margin measurement accuracy and for configuring margin reduction aggressiveness~\cite{mericas2014performance,webel2015robust,berry2018ibm}, an example is Power Supply Monitor (PSM) on AMD processors~\cite{grenat20145}. The figure exposes the pros and cons of different approaches.

Starting with the baseline where there is no active timing margin, under a chip-wide static margin (i.e., first bar), all cores have a fixed frequency of 4.2~GHz. Setting the static margin for each core (second bar) with fixed $\textless$$v,f$$\textgreater$ improves performance by exposing the fast cores; we estimate the fastest cores can run around 4.5~GHz, based on prior art's voltage noise characterization~\cite{zu2015adaptive}.

Next, the default active timing margin (third bar) carefully programs each CPM to provide uniform core performance, following the conventional contract between processors and users. When idle, all cores run near 4.6~GHz, higher than static margin's fastest cores because of active timing margin's highly effective mitigation of $di/dt$ effects~\cite{lefurgy2011active}. However, when high power workloads are run, the induced DC voltage drop across the power delivery grid can create long-term steady degradation of the supply voltage delivered, eroding timing margin and reducing active timing margin's frequency gain~\cite{zu2015adaptive}, which lowers the worst-case performance to around 4.4~GHz. Setting fixed $\textless$$v,f$$\textgreater$ points for each core requires that this worst-case be guarded against, whereas active timing margin handles it adaptively and frequency only suffers when power consumption is high.

Fine-tuning (fourth bar) at the per-core adaptive clocking control loop level exposes similar inter-core speed variation as static per-core $\textless$$v,f$$\textgreater$ setpoints, but it provides much higher performance under typical conditions because of active timing margin's adaptive margin provisioning capability. Fine-tuning active timing margin also removes any margin left not trimmed in the default system, which further pushes processor efficiency to the extreme. For instance, when the chip is idle, power consumption and DC voltage drop is minimal, pushing the fastest core to nearly 5~GHz, 10\% higher than the fastest static margin core.

While fine-tuning active timing margin provides high frequency gain, it exacerbates variability and induces performance predictability issues. In the worst case, e.g., when DC voltage drop is maximized when running eight high power {\bench{daxpy}} threads, the slowest core, which runs at 4.7~GHz under idle conditions, slows down to 4.5~GHz, a 500~MHz drop from the fastest 5~GHz case. Thus, application performance can vary widely, depending on the core chosen for execution and any co-located workloads.

\begin{figure}[t]
  \centering
  \includegraphics[trim=0 0 0 0,clip,width=.75\linewidth]{graphs/process/noisy-latency.pdf}
  \caption{{\bench{SqueezeNet}} inference latency on a POWER7+ core under different timing margin settings. Aggressively fine-tuning active timing margin, and co-locating it with ``friendly'' low-power applications significantly enhance performance.}

  \label{fig:motivate-latency}
\end{figure}

\Fig{fig:motivate-latency} shows a POWER7+ processor core's performance under different timing margin settings~\cite{sinharoy2011power7, floyd2011introducing}. We instrument POWER7+'s active timing margin via its Critical Path Monitors (CPMs), a programmable interface of the chip's canary circuit that measures available margin~\cite{lefurgy2011active, drake2013single}. We illustrate with the inference latency of \bench{SqueezeNet}, a compressed convolutional neural network (CNN). Under conventional static timing, the chip clocks at 4.2~GHz, producing an average inference latency of 80~ms. Under the chip's default active timing margin, a poorly managed system that co-locates \bench{SqueezeNet} with high-power co-runners such as \bench{daxpy} increases frequency to 4.4~GHz, yielding a limited 7.5\% latency improvement. However, customizing each core's active timing margin and wisely managing the system to let \bench{SqueezeNet} run alone boosts core frequency to 5~GHz and reduces latency by 15\%, a 2X the performance gain over the default production system.

Inspired by the benefits shown in~\Fig{fig:motivate-latency}, this chapter detail how to fine-tune active timing margin at the core-level to robustly reveal each core's performance limit and to expose inter-core speed differences. We perform extensive hardware measurement to analyze active timing margin's operating limits under different application scenarios, which leads to a low-overhead solution for deploying active timing margin systems with their highest speed at scale, while delivering controllable application performance in the presence of the exposed process and voltage variation. We present a software solution to actively fine-tune and manage active timing margin. In summary, we make the following contributions:

\section{Fine-tuning Core-level Active Timing\\ Margin Operation}
\label{sec:process:configurability}

In our study, we convert all of active timing margin's reclaimed timing margin into frequency and keep $V_{dd}$ unchanged. This process bypasses the restriction on undervolting wherein a chip's worst-case core limits the amount of undervolting. Overclocking allows each core to independently adapt to its conditions and can fully expose a chip's inter-core speed differential, potentially producing more performance benefit. We let active timing margin boost each core's frequency at $V_{dd}$ 1.25~V, the 4.2~GHz P-state.

We explain how to customize a multicore's active timing margin operation to be more aggressive, which extracts more timing margin and increases frequency. Reconfiguring active timing margin's control loop to its operating limit is unexplored before, thus we propose a systematic procedure to characterize how the processor behaves under different scenarios and timing margin reclamation levels. The insights we gain when executing this procedure is instrumental in deploying customized active timing margin systems in production.

\subsection{Programming Critical Path Monitors \\to Reconfigure Margin Reclamation}
\label{sec:process:configurability:howto}

We configure the POWER7+'s Critical Path Monitors (CPMs) to fine-tune active timing margin's margin reclamation behavior. By design, CPMs are programmable to set how aggressively active timing margin trims the margin and, more importantly, to cover speed variation and deliver uniform performance to users. We leverage this interface to fine-tune each core's active timing margin control loop.

\begin{figure}[t]
  \centering
  \includegraphics[trim=0 130 0 130,clip,width=0.8\linewidth]{graphs/process/cpm-struct.pdf}
  \captionsetup{width=.95\linewidth}
  \caption{CPM has three cascaded parts: programmable inserted delay, synthetic paths, and inverter chain.}
  \label{fig:cpm-struct}
\end{figure}

\Fig{fig:cpm-struct} shows a CPM uses three stages to measure margin~\cite{drake2007distributed, drake2013single}: (1)~inserted delay, (2)~synthetic paths, and (3)~an inverter chain. The inserted delay is a configurable circuit. A user can specify the number of inverters a signal passes through to select its timing delay length. The synthetic path simulates a pipeline circuit's delay with a set of paths, including AND, OR, and XOR gates and wires. The final inverter chain quantifies the time left after the signal propagates past the inserted delay and synthetic path by counting the number of inverters a signal passes. The inverter count is a CPM's final output and is sent to the DPLL for clock adjustment.

Before a POWER7+ processor is shipped, each CPM's inserted delay is pre-set at test-time with a default value that serves as extra ``protection'' for the control loop to function robustly. The pre-set delay makes CPMs report less margin than it could have, leaving some margin not trimmed as protection. The pre-set delay also smooths out the speed differences between different corners of a chip by adding more delay to fast corners in order to fill the empty time after a circuit finishes switching and adding less delay to slow corners.

\begin{figure}[t]
  \centering
  \includegraphics[trim=0 0 0 0,clip,width=0.95\linewidth]{graphs/process/default-cpm.pdf}
  \captionsetup{width=.95\linewidth}
  \caption{Pre-set inserted delay of the CPMs in two POWER7+ chips, grouped by core. There exists wide variation between different CPM sensors.}
  \label{fig:default-cpm}
\end{figure}

\Fig{fig:default-cpm} shows the preset inserted delays in each core of the two POWER7+ chips (we exclude CPMs in the LLC because it lies in a different clock domain). Intuitively, each unit of the delay represents some amount of timing. Under static margin at 4.2~GHz, reducing the inserted delay by one step lets the CPM detect one to three units more timing margin, equivalent to the speed variation caused by 20-60~mV $V_{dd}$ difference~\cite{drake2013single,zu2015adaptive}. The magnitude of the preset delay shows the amount of ``protection'' built into the default active timing margin system. The pre-set inserted delays range from 7 to 20, nearly a 3X range, indicating significant silicon speed variation.

\begin{figure}[t!]
  \centering
  \includegraphics[trim=0 0 0 0,clip,width=0.8\linewidth]{graphs/process/delay-freq.pdf}
  \caption{Reducing inserted added delay makes the CPM count more time margin after a signal travels through the synthetic path. The DPLL then increases frequency to harvest the excess margin reported by CPM's inverter chain.}
  \label{fig:delay-freq}
\end{figure}

By programming the inserted delay to different values, active timing margin's perception of the amount of available timing margin changes, and thus it is induced to become more or less aggressive in reclaiming timing margin. \Fig{fig:delay-freq} shows, for four example cores (C), across two processors (P) on the same system, how active timing margin converts more margin into frequency as the CPM inserted delay is reduced. The default delay (normalized to 0) makes active timing margin push core frequency to around 4.6~GHz, but reducing inserted delay (reduction steps beyond 0) pushes frequency to over 5~GHz, a 20\% improvement over the static timing margin baseline. Programming the inserted delay to a smaller value (higher delay reduction) decreases the time to the end of the synthetic path, leaving more margin to be counted by the inverter chain. The DPLL loop harnesses the excess margin by overclocking.

Before a POWER7+ processor is shipped, each CPM's inserted delay is configured with some default ``protection'' delay to keep the CPM timing margin conservative, which guarantees correct active timing margin execution. The protection delay also smooths out the speed differences between different corners of a chip. For the 64 CPMs in our two-socket system (we exclude CPMs in the LLC because it lies in a different clock domain), the protection delays range from 7 to 20, nearly a 3X range, indicating significant silicon speed variation.

In the POWER7+, we configure the inserted delay by programming it with a discrete step count through the server's accompanying service processor. Each step represents some amount of timing delay. Under the static margin at 4.2~GHz, reducing the inserted delay by one step lets the CPM detect one to three units more timing margin, equivalent to the speed variation caused by 20-60~mV $V_{dd}$ difference~\cite{drake2013single,zu2015adaptive}.

We reduce each core's CPM delay from the default amount to increase active timing margin aggressiveness. To simplify the exploration space, we reduce the four CPMs within a core (excluding the LLC CPM) by the same amount.

\subsection{Characterizing active timing margin Limits}
\label{sec:process:methodology}

As shown by \Fig{fig:delay-freq}, active timing margin has great potential for more aggressive operation to achieve higher frequency. But to unlock active timing margin's full potential, we need a methodology to characterize the system. \Fig{fig:methodology} outlines our procedure. 

We profile an active timing margin chip on a per-core basis. System idle is our starting point for the analysis; micro-benchmarks (uBench) cover major paths in a core; and single-threaded benchmarks representing real use cases.

\textbf{System Idle} Running background operating system tasks, an idle system imposes the least stress on the processor. {Understanding each core's active timing margin operating limits under system idle provides us with valuable insight into inherent core-to-core differences.}

\textbf{Micro-benchmarks (uBench)} Traditionally, micro-benchmarks are used to measure the performance of individual processor modules, such as the branch predictor, floating point unit, and caches. In active timing margin, micro-benchmarks serve an additional purpose because each one primarily touches only one part of the core, avoiding complex microarchitectural interactions. We thus use uBench to get a more comprehensive profile of core-to-core microarchitecture level variation.

\begin{figure}[H]
  \centering
  \includegraphics[trim=10 280 10 280,clip,width=.75\linewidth]{graphs/process//profile-flow/profile-flow.pdf}
  \caption{Our active timing margin characterization methodology iterates over each core and follows a step-by step approach, going from the simplest system idle scenario to the complex real-world workloads.}
  \label{fig:methodology}
\end{figure}

\textbf{Realistic Workloads} %\paragraph{Single-threaded Benchmark}
For the final step, we profile the system with complex applications from SPEC CPU 2017 and PARSEC. These benchmarks cover a wide spectrum of program space in the real world and have diverse architecture behavior~\cite{song2018spec,bienia2008parsecsplash}; hence they can touch more corner-case timing paths or create more active $di/dt$ effects than uBench, all of which threatens the safe execution of aggressively reconfigured active timing margin. {The single-threaded workloads help identify application, chip-wide, and individual core level heterogeneity.}

In each of the above setups, failure may occur as a result of timing violation, manifested as an abnormal application termination (e.g., segmentation fault), silent data corruption (SDC), or a system crash. For SDC related error, we rely on SPEC and uBench's inherent result checking tool for guaranteeing execution correctness. All these failures may occur because either the CPM's delay has become so short that it does not capture real circuit delays or system noise events, such as the $di/dt$ effect, overwhelms the control loop's ability to respond in time. Because the effects that cause active timing margin failure might be not fully deterministic, we repeat the profiling in each setup at lest 20 times to produce a distribution of active timing margin operating limits. We expect the distributions to be tight because timing violations will not be entirely random. These distributions provide a holistic view of active timing margin's margin reclamation capability, so we study them from here on.

Our methodology progresses through increasing workload complexity. Thus we often need to roll back the CPM delay setting that was successful in a previous less complex setup to a less aggressive point, reflecting a workload setup's unique impact on active timing margin's operation. Although the worst-case scenario might determine practical active timing margin reconfiguration in the real world, the middle point analysis shed useful insights on what affects the core-level customization of active timing margin's margin reclamation.

There is no guarantee that a particular circuit path or system noise event will deterministically lead to a timing violation, so we repeat the profiling in each of the above setups at least 20 times to produce a distribution of active timing margin operating limits. On the other hand, the effects that lead to a timing violation are not entirely random. Reconfiguring CPM inserted delay beyond a limit often leads to certain critical paths having much higher probabilities of experiencing timing errors; thus, the resulting distributions of successful CPM delays tend to be very tight. These distributions provide a holistic view of active timing margin's margin reclamation capability, so we study distributions here onward.

A timing violation manifests as an abnormal application termination (e.g., segmentation fault) or a system crash. It happens because either the CPM's delay has become so short that it does not capture real circuit delays, or system noise events, such as the $di/dt$ effect that overwhelms the DPLL.
%A timing violation manifests as either an abnormal application exit (e.g., early termination) or a system crash. These failures occur either because (1)~the removal of the CPM protection delay gives rise to a circuit path somewhere with a longer delay than that captured by the CPM; or (2)~system noise events, such as turbulence on the $V_{dd}$ power supply plane, undermine DPLL's ability to quickly slew frequency. 

Our profiling methodology progresses through increasingly complex workloads. Thus we often need to roll back the CPM delay setting to a less aggressive point, reflecting a workload's unique impact on active timing margin's operation.
%Each step of our profiling methodology builds upon the previous, less complex setup and so may result in failure where the previous step was successful. Thus we often need to roll back active timing margin operation to a less aggressive reclamation point by reconfiguring CPM inserted delays. The rollback delta reflects the workload's unique impact on active timing margin's aggressive operation.

\section{Idle System Characterization}
\label{sec:process:idle}

Understanding active timing margin's margin reclamation limits in an idle system sets a starting point for further, more complex analysis. With no application code running, the system exerts minimal stress on active timing margin's reconfigured control loop, enabling us to use active timing margin to expose the silicon's inherent maximum speed.

%We leverage the CPM delay reduction results in \Fig{fig:delay-freq} and incrementally reconfigure active timing margin to be more aggressive until the system fails. In this setting, failure occurs either because CPM reconfiguration removes the system's built-in protection delay and could give rise to a circuit path somewhere for which the CPM cannot adequately represent its timing delay or because system noise events such as turbulence on $V_{dd}$'s power supply plane makes the DPLL fail to slew frequency in time. Nevertheless, under many CPM reconfigurations, the more aggressive active timing margin still yields flawless execution.

Running only the operating system, we build a distribution of the most aggressive yet safe CPM configuration points for each core, depicted in \Fig{fig:idle-limit-dist} by the amount of CPM delay reduction from the chip's default setting, along with the resulting frequencies. As expected, the distributions are tight, covering no more than two configurations. Each core's \textit{idle limit} is the lowest (most conservative) CPM delay reduction plotted, e.g. 9 in \Fig{fig:idle_dist_p0c0}. These are summarized in \Tab{tab:limit-delay}. 

\input{tex/process_submodules/tab_limit-delay}

The different core-to-core idle limits reveal lucrative performance potential for aggressive active timing margin customization (\Sec{sec:process:idle:potential}), and the significant core-to-core performance variation (\Sec{sec:process:idle:heterogeneity}) which is partly caused by the non-linearity in CPM configuration (\Sec{sec:process:idle:cpm_variation}).

\input{tex/process_submodules/fig_idle-limit-dist}

\subsection{Significant Performance Potential}
\label{sec:process:idle:potential}

For most cores, the inserted delay can be aggressively reduced, making active timing margin's control loop see more timing margin for reclamation. As \Fig{fig:idle-limit-dist} shows, more than half the cores (e.g., P0C0 and P0C1) can tolerate reductions of at least seven steps of CPM inserted delay, elevating frequencies to over 5000~MHz: a 7\% improvement over default active timing margin's 4600~MHz and a 20\% improvement over static margin's 4200~MHz baseline, showing customized active timing margin can substantially improve performance.

\subsection{Exposed Inter-core Frequency Variation}
\label{sec:process:idle:heterogeneity}

Programming the CPM to change active timing margin operation yields different frequency levels for each core, despite the performance improvement. For instance, at the idle limit P1C2 runs at about 4850~MHz but P0C3 achieves about 5200~MHz. Even within a chip, there is a wide range (e.g., P0C2 and P0C3). The core-to-core frequency variation is essential for application performance management, which we discuss later.

The core to core differences are understood to be a result of manufacturing process variations~\cite{dighe2010within,rangan2011achieving}, i.e., some core's circuits are faster due to imperfection in the lithography process. For instance, as \Fig{fig:idle-limit-dist} shows, P0C3 can safely reduce its CPM delay by 11 steps, while P0C7 can only mitigate its delay by two, reflecting the varying amount of timing margin available for reclamation, which is caused by the two cores' speed difference.

However, because on the POWER7+ each core's performance potential is unlocked via active timing margin control loop's automatic harness of available timing margin, the functioning of active timing margin control loop also plays a critical role in the inter-core performance variation.

\subsection{Nonlinearity of CPM Configuration}
\label{sec:process:idle:cpm_variation}

The CPM inserted delay's configurable inverter chain is designed to have linear timing delay graduation for timing margin measurement. However, the manufacturing process makes it have non-linear graduation when configured to measure timing margin. The non-linearity magnifies the inter-core performance heterogeneity.

The inserted delay's non-linear configuration manifests as significant idle limit variation between cores. Consider P0C4 and P1C7, which are both able to increase frequency from 4600~MHz to 5100~MHz but do so with very different CPM changes: P0C4 reduces the delay by ten steps, while P1C7 only needs two steps. Hence, although the two cores have similar excess timing margins, P0C4's CPM encodes smaller timing delays in each step than P1C7. 

Within each core, CPM's non-linearity makes the timing margin encoded by one CPM delay step vary. \Fig{fig:delay-freq} shows that P1C6's frequency increases by over 200~MHz when going from step zero to one, jumping from the baseline 4600~MHz to over 4800~MHz. But in going from step one to two, there is an almost negligible change in frequency. Similarly, the frequency is nearly unchanged when increasing the CPM delay reduction from step five to six for P1C3, but reducing the delay by one additional step (i.e., going from six to seven) increases the frequency by over 100~MHz.

As another example, in \Fig{fig:idle_dist_p1c2} reducing P1C2's CPM delay by six is too aggressive and can crash the system; rolling back its delay by one step ensures safety but at the cost of 300~MHz. P1C1 (\Fig{fig:idle_dist_p1c1}) similarly needs its CPM delay reduction rolled back by one step (from nine to eight) for safe operation but at the cost of only 100~MHz. Though P1C2 could operate safely at a higher frequency, the large CPM jump forces the 300~MHz drop and amplifies the differences between the two cores.

In summary, the non-linearity configuration of the CPM and active timing margin control loop demands that customization of multi-core Active Timing Margin operation be carried out carefully on the per-core basis because no single CPM configuration works uniformly for all cores.

\section{Micro-bench Characterization}
\label{sec:process:ubench}

\input{tex/process_submodules/fig_ubench-limit-dist}

While idle system characterization reveals insights on the performance benefits and the inter-core variation issues of multicore active timing margin customization, it does not evaluate the system's behavior under stress from real-world application codes. Before using more complex applications, we use micro-benchmark (uBench) as a valuable tool that controls program behavior to analyze individual processor components~\cite{papadimitriou2018micro}. Because uBench imposes more stress than idling, the CPM configuration tends to be more conservative, creating a practical point for processor deployment in the real world.

\subsection{Workload Selection}
\label{sec:process:ubench_benchmarks}

We evaluate system behavior under aggressive active timing margin customization using three uBench programs. These programs collectively cover all main parts of the microarchitecture, as well as the dispersed CPMs in a core. 

We use \bench{coremark}~\cite{coremark} to stress the core's control, branch, and integer units; \bench{daxpy} to stress the floating point unit; and \bench{stream}~\cite{stream} for its ability to generate cache misses and exercise the load-store unit. Prior work has used such benchmarks to exercise the functional units and validate the active timing margin~\cite{lefurgy2011active, lefurgy2013active}. We check the programs' run result to evaluate processor execution correctness. All incorrect runs manifest as system crashes or abnormal application exits.

Using these benchmarks ensures we challenge a reconfigured active timing margin by touching more paths than system idle. Meanwhile, these uBench programs create little system noise, especially the $di/dt$ effect. They have controlled, smooth program behaviors and avoid complex microarchitectural activity such as periodic pipeline flush, which is the root cause of workload-induced voltage droops~\cite{grochowski2002microarchitectural,powell2003pipeline,reddi2009voltage,reddi2010voltage,miller2012vrsync}. The $di/dt$ effect is dangerous for aggressively reconfigured active timing margin because its fast drooping voltage can prevent the control loop from engaging in time~\cite{vezyrtzis2018droop}, resulting in application failure. 

\subsection{What Makes Some Cores Fail?}
\label{sec:process:ubench_profiling}

We start the uBench characterization from the idle limit because it is the point that sustains stable system state. If this initial starting point fails, the CPM inserted delay is rolled back to have a longer timing delay to make active timing margin harness timing margin more conservatively until the program runs correctly. We find most cores' idle limits sustain correct uBench execution, which entails they can safely accommodate the major paths activated by the instructions used by uBench programs. 

For the server's two physical processors, uBench characterization exposes six cores that fail for the three programs. \Fig{fig:ubench-limit-dist} shows the distributions of reintroduced delays for these cores, using the ``rollback steps'' relative to the idle limit, which reflects the stress impact from uBench program execution compared with system idle. For those six cores, rollback ranges from one to three steps and sustains all uBench workloads.

All three programs, despite their different characteristics, show similar behaviors on the six problematic cores. The implication is that the microarchitecture blocks that limit active timing margin fine-tuning are the common structures used by all programs, such as instruction fetch and scheduling, rather than specific modules stressed by each application (e.g., FP unit). We also find \textit{uBench limit} sustains voltage and power stress-test, which will be detailed later in this paper. We, therefore, use the \textit{uBench limit} as a reference point for further characterization using realistic applications.

%All three programs, despite their different characteristics, show similar behaviors on the six problematic cores. The implication is that the microarchitecture blocks that limit aggressive active timing margin customization are the common structures used by all programs, such as instruction fetch and scheduling, rather than specific modules stressed by each application (e.g., FP unit).
%Our later analysis with a power virus and a voltage virus corroborates this conjecture. Neither of these stress tests makes the cores fails at their \textit{uBench limit}.
%The power virus creates a high IR drop and high-temperature condition. At \textit{uBench limit} in \Tab{tab:limit-delay}, eight copies of \bench{daxpy} threads creates over 160~W total chip power and around 70\C chip temperature, compared with the 50\C temperature under per-core test. However, the high power does not bring any core down, verifying the robustness of \textit{uBench limit}. 
%We believe this observation is intuitive because the temperature's impact on circuit speed happens over the long term, which is well within active timing margin control loop's nanosecond-level response time. A higher temperature is beneficial for circuit speed in recent technologies~\cite{zu2016tistate}, reducing concerns on high-temperature conditions.
%The voltage virus creates transient voltage droops that threaten the active timing margin control loop to respond in time. We repeatedly throttle all cores' instruction issue rates to its 1/128 in synchronized step while running \bench{daxpy} to create current surges, which induces worst-case $di/dt$ effect~\cite{lefurgy2011active, lefurgy2013active}. However, under this worst-case condition, no core fails at their \textit{uBench limit}, suggesting active timing margin works fairly robustly under aggressive customization. We, therefore, use the uBench limits as a reference point for further characterization using realistic applications.

% The uBench limit is an important configuration that likely reflects the core's inherent active timing margin achievable speed because most timing paths are protected by aggressively configured active timing margin operation and real programs have high confidence of correct execution.

\section{Realistic Workload \\Characterization}
\label{sec:process:realistic}

To run real applications, a production active timing margin system today adds some amount of protection margin to CPM's uBench limit configuration~\cite{lefurgy2011active}. To conservatively guarantee execution correctness, the added margin can be up to 50\% of the static guardband. But this leaves room for improvement as demonstrated by the 2X frequency gain during our system idle characterization. 

However, adding additional guardband as a conservative precaution ignores the application-dependent behavior and can waste valuable performance benefit. In this section, we profile with a variety of integer and floating point workloads from SPEC CPU 2017~\cite{SPEC2017} and PARSEC~3.0~\cite{bienia2008parsec}. We use these workloads because their result-checking tool provides a convenient method for checking execution correctness. Understanding per core active timing margin operating limits under these heterogeneous workloads offer helpful insights for deploying aggressively customized active timing margin chips in real-world use cases.

To understand all system factors that impact an aggressively fine-tune active timing margin processor, we profile with a variety of integer and floating point workloads from SPEC CPU 2017~\cite{SPEC2017} and PARSEC~3.0~\cite{bienia2008parsec}. These realistic workloads provide helpful insight for deploying aggressively fine-tuned active timing margin chips in real-world use cases. They often have more complicated code patterns that may touch corner timing paths in a core, or introduce complex microarchitectural behaviors that can lead to severe $di/dt$ effects, both of which threaten to violate the aggressively tuned CPM configuration after uBench profiling, even though the uBench limits already ensure the active timing margin control loop protects major core paths. 

%We find that SPEC and PARSEC benchmarks usually fail at the uBench limit; this is why active timing margin processors that are deployed into the field still rely on some safety margin, approximately 50\% of the original static guardband~\cite{lefurgy2011active}, after the CPMs have been calibrated using the uBench programs. Therefore, there is still substantial room for improvement.
%The delta between each application's limit CPM configuration and the uBench limits reveal the unique impact of an application's system noise effect. Most of the workloads require that each core roll back its CPM delay from the uBench limit by at least one step to ensure execution correctness. More importantly, we observe different applications impose widely different ``stress'' levels on the aggressively configured active timing margin chip. 

\subsection{Application Heterogeneity}
\label{sec:process:workload:heterogeneity}

\begin{figure}[t]
    \subfloat[P0C3] {
        \includegraphics[trim=0 0 100 0,clip,width=0.4\linewidth]{graphs/process//spec-limit-dist/single-thread-cmp-p0c3.pdf}
        }
    \hfill
    \subfloat[P1C6] {
        \includegraphics[trim=100 0 0 0,clip,width=0.4\linewidth]{graphs/process//spec-limit-dist/single-thread-cmp-p1c6.pdf}
        }               
    \caption{\bench{x264} stresses active timing margin more heavily and needs a more conservative CPM configuration compared to \bench{gcc}, as indicated by the larger CPM rollback that is required for \bench{x264} over \bench{gcc}.}
    \label{fig:spec-limit-example} 
\end{figure}

\Fig{fig:spec-limit-example} shows \bench{x264} often requires significant CPM delay rollback from the uBench limit, whereas \bench{gcc} needs relatively little, allowing active timing margin to more aggressively boost frequency. The rollback reflects an application's unique system noise effects. Configuring active timing margin for the worst application in all cases, e.g., \bench{x264}, wastes active timing margin's margin reclamation potential when running more benign workloads. This is the approach taken by today's deployed active timing margin processors, which still rely on a safety margin as large as 50\% of the original static guardband~\cite{lefurgy2011active}. This is the case for today's active timing margin processors deployed into the field which still rely on some safety margin, approximately 50\% of the original static guardband~\cite{lefurgy2011active}.

To get a complete picture of the behavior of aggressively configured active timing margin cores on different workloads, we profile CPM rollback from the uBench limit for all $<app, core>$ pairs in \Fig{fig:app-cpm-heactive timing marginap}. We use the weighted average CPM rollback as it quantifies the application's unique stress level. Two applications may have quite a different delay reduction distributions even when they show the same lower bound in their CPM delay profile. 

From the individual rows in \Fig{fig:app-cpm-heactive timing marginap}, we see that each workload imposes a different amount of stress but does so consistently across cores. For instance, \bench{x264} and \bench{ferret} needs much more conservative active timing margin setting than \bench{gcc} and \bench{leela}, indicating these workloads have exerted a higher pressure on active timing margin's control loop.

We classify the workloads as ``heavy,'' ``medium,'' or ``light'' as shown in \Tab{tab:bench-cpm-type}. ``Heavy'' workloads pose the greatest threat to aggressively reconfigured active timing margin and often force a rollback of CPM inserted delay for more conservative operation. In contrast, ``light'' applications exert little pressure on active timing margin and often need no rollback from the uBench limit. The ``medium'' workloads show more sensitivity to a core's active timing margin control loop.

In \Tab{tab:limit-delay}, \textit{thread-worst} is the worst CPM configuration limit of all workloads and represents the most severe application stress in our profiling. The \textit{thread-normal} is less conservative and lets most medium, and light applications safely pass. From our realistic single-threaded workload profiling, we draw the following two key insights:

\begin{table}[t]
  %TODO: this table can be removed if space is needed
  \vspace{0.2cm}

  \centering
  \begin{tabular}{l|c*{2}{c}}
    \Xhline{1pt}
    stress level & benchmark \\
    \hline
    heavy  & \makecell{x264, exchange2, ferret} \\
    \hline
    medium & \makecell{perlbench, xalancbmk, xz, \\facesim, omnetpp, mcf, \\bodytrack, dedup} \\
    \hline
    light  & \makecell{gcc, bodytrack, deepsjeng, leela, \\freqmine, barnes, streamcluster, \\fluidanimate, fft, blackscholes} \\
    \Xhline{1pt}
  \end{tabular}

  \caption{Benchmark classification based on their stress level to aggressively configured active timing margin.} 
  \label{tab:bench-cpm-type} 
\end{table}

From the individual columns in \Fig{fig:app-cpm-heactive timing marginap}, we see that different cores exhibit varying levels of ``robustness'', where we define robustness as the immunity to CPM rollback from the core's CPM uBench limit. The cores on the right of~\Fig{fig:app-cpm-heactive timing marginap} has the highest robustness, requiring the least rollback across all applications, indicating their active timing margin control loops can deal with the system effects of any application. We anticipate they will continue to be robust on untested applications since the profiled workloads already cover different behaviors~\cite{song2018spec}.

The reason why certain applications and cores are more vulnerable after aggressive active timing margin customization is a combination of the core's inherent speed and the running application's characteristics. We conducted a best-effort static instruction analysis on the applications and concluded that more detailed insight into the running instructions is needed to predict each application's best-fit CPM setting on each core. For instance, \bench{gcc} covers a much richer set of instructions than \bench{exchange2}, likely touching more corner timing paths, yet stresses active timing margin much less. As another example, \bench{x264} has similar performance counter profiles as \bench{leela}, but their rollback requirements differ substantially. We, therefore, defer the root-cause analysis and the prediction of applications' heterogeneous CPM configuration to future work and focus on the variations already exposed.

\begin{figure}[t]
  \centering
  \includegraphics[trim=0 0 0 0,clip,width=\linewidth]{graphs/process//spec-limit-dist/app-rollback.png}
  \caption{Application's average CPM delay rollback from the core's uBench limit. The top workloads stress active timing margin heavily and need more delay rollback for less aggressive margin reclamation.}
  \label{fig:app-cpm-heactive timing marginap}
\end{figure}

\subsection{Core Robustness Heterogeneity}
\label{sec:process:workload:robustness}

%Looking horizontally in~\Fig{fig:app-cpm-heactive timing marginap}, different cores, cores exhibit varying active timing margin control ``robustness''---under complex workload-induced system noise, the amount of CPM delay needed to roll back from the core's inherent speed which is exposed by the uBench limit profile. In \Fig{fig:app-cpm-heactive timing marginap}, the cores on the right exhibit the highest robustness because they need the least rollback across all applications, indicating their active timing margin control loops can deal with the system effect caused by whatever application fairly well. 

Cores have varying levels of ``robustness'' to application heterogeneity, where we define robustness as the immunity to CPM rollback from the core's inherent speed (the uBench limit profile). From the columns in \Fig{fig:app-cpm-heactive timing marginap}, the cores on the right exhibit the highest robustness, requiring the least rollback across all applications, indicating their active timing margin control loops can deal with the system effects of any application.

\Fig{fig:core-cpm-var} sorts cores' average rollbacks across all workloads. The rightmost cores, P0C7, P1C2, and P1C7 are immune to workload effects, flawlessly executing all applications at their uBench limit. We anticipate they will continue to be robust on untested applications since the profiled workloads already cover various behaviors~\cite{song2018spec}. These ``robust cores'' can be relied upon in a production environment to execute any application. Among the robust cores, P1C7, however, is notable because its CPM delay was rolled back from the idle test to the uBench test, significantly reducing its frequency to a rather conservative 4800~MHz, possibly accounting for its apparent robustness. Contrariwise, P0C7 remains robust even at its CPM delay from the idle test. As such, there is no clear correlation between a core's speed and its active timing margin robustness. 

%To favor execution reliability, the cores that have more robust active timing margin operation on the right side of \Fig{fig:core-cpm-var} have higher priority for running applications.

\Fig{fig:core-cpm-var} also summarizes different cores' frequency variation under the profiled scenarios. At the uBench limit configuration, core-to-core speed varies by 300~MHz from the fastest, P0C6, to the slowest, P1C7. The speed gap shrinks to 200~MHz at the thread-worst limit, caused by CPM delay rollback of the non-robust active timing margin cores. Nevertheless, the non-uniform core frequency is still impressive and deserves proper management. The arithmetic mean frequency is 4908~MHz, under thread-worst setting, and the standard deviation is 63.5~MHz.

%If alternatively selected to favor performance, faster cores should be given higher priority for running applications.

\begin{figure}[t]
  \centering
  \includegraphics[trim=0 20 0 20,clip,width=\linewidth]{graphs/process//spec-rollback.pdf}
  \caption{Aggressively configured active timing margin cores exhibit different CPM rollback steps and frequencies when running realistic workloads.}
  \label{fig:core-cpm-var}
\end{figure}

\section{Managing Fine-tuned Active Timing Margin}
\label{sec:process:schedule}

In this section, we discuss how to deploy and manage a fine-tuned active timing margin system into the field in the presence of significant variability. Fine-tuning improves application performance because frequency is higher. However, pushing active timing margin to its operation limit requires execution correctness guarantee, and the varying frequency levels of different cores and application scenarios create obstacle for the processor to delivering a promised performance level to end users. Hence, we discuss how to determine CPM settings for each core to robustly expose variation, and show how to schedule and throttle co-running workloads to deliver predictable performance for latency sensitive critical applications.

\subsection{Deploying Fine-tuned Active Ting Margin Configuration}
\label{sec:process:schedule:deploy}

The insights we gather while analyzing the operating limits of a fine-tuned active timing margin system under idle, micro-benchmarks, and realistic workload scenarios are useful for understanding the performance opportunity from exposed inter-core variability, but overhead of our procedure is too high for finding a processor's fine-tuned configuration in a real-world deployment.

Because programs have heterogeneous CPM settings on different cores, one might try to predict each application's best CPM setting on each core. However, such a prediction scheme would demand essentially perfect prediction accuracy because any misprediction can lead to system failure or incorrect execution. Achieving this accuracy is difficult because it relies on deep knowledge of a program's $di/dt$ behavior as well as the circuit paths touched by the program, all of which derives from the dynamic instruction streams and may incur high overhead~\cite{reddi2009voltage}. We leave CPM prediction for future work.

Rather than predict CPM settings, we propose a test-time stress-test procedure to identify active timing margin fine-tuning limits while maintaining a correctness guarantee. The approach and evaluation presented here is an example of the process we recommend, and not meant to be literally the exact steps to follow. For instance, the stressmarks we use in the paper are different from what we use in production. Nonetheless, the general approach we discuss is useful.

During test-time, we iterate over each core and run worst-case workloads, such as a $di/dt$ stressmark~\cite{kim2012audit,bertran2014voltage}, power stressmark~\cite{bertran2012systematic}, and ISA test suites to create high voltage noise and high operating temperatures and to cover all potential circuit paths. The combined stress-test finds each core's limit active timing margin configuration, providing a guarantee of correctness for any realistic workload because, by definition, a stress-test pushes the system beyond the requirements of any other workload.

In our work, we try our best to create a stress-test with a voltage virus that repeatedly and synchronously throttles all cores' instruction issue rate to operate only one out of every 128 cycles while simultaneously running 32 {\bench{daxpy}} threads. The {\bench{daxpy}} workloads create high power consumption, raising chip power to 160~W and temperature to 70\C; the issue throttling creates a synchronous power surge across the chip, inducing concurrent $di/dt$ effects from adjacent cores, representing worst-case voltage noise ~\cite{lefurgy2011active,vezyrtzis2018droop}. We recognize that better power stressmarks can be constructed using more systematic procedures~\cite{bertran2012systematic}, but we do not expect power and temperature to be the limiting factors for active timing margin operation because these are long-term effects and are well within active timing margin control loop's nanosecond-level response time. Though the realistic workload characterization in \Sec{sec:process:realistic} covers a variety of instructions, in practice, chip vendors have tailored ISA verification suites that provide wider coverage and execute in less time.

\begin{figure}[t]
  \centering
  \includegraphics[trim=0 50 0 0,clip,width=.85\linewidth]{graphs/process/freq-variation.pdf}
  \caption{To ensure execution correctness, fine-tuning active timing margin goes through worst-case stress-test during test time. Vendors can optionally roll back stress-test active timing margin configurations, providing additional safety guarantee. Either way, speed variability is exposed.}
  \label{fig:freq-var}
\end{figure}

On the two tested POWER7+ chips, the \textit{thread worst} CPM configurations sustain correct execution under all our stressmarks. To provide additional correctness guarantees, the vendor can optionally rollback the stress-test-determined active timing margin limit by several steps.

\Fig{fig:freq-var} shows the core frequencies across the two POWER7+ chips after executing the above test-time procedure. At their limit, P0C1 and P0C7 have over 200~MHz speed differential. Rolling back each core's CPM from the limit by one or two steps keeps the same inter-core variation trend and provides an additional safety guarantee. In the management scheme we propose, we will use the limit \textit{thread-worst} configuration, though the conclusions we present and the scheme we propose can be applied to more conservative (rolled back) configuration points.

\subsection{Per-core Frequency Predictor}
\label{sec:process:schedule:predict}

To manage active timing margin's performance variability, we first develop a predictor that informs frequency and performance for a candidate application schedule. We develop this predictor by modeling each core's runtime average frequency $\overline{f}$, as a linear function of the transistors' supply voltage, $V_{chip}$. Among different dynamic effects, long-term stable effects such as temperature variation and DC voltage drop caused by high power determines core frequency ---infrequent, transient $di/dt$ events are handled transparently by the active timing margin control loop. 

\begin{figure}[t]
  \centering
      \includegraphics[trim=0 0 0 0,clip,width=.6\linewidth]{graphs/process/freq-pred.pdf}
      \captionsetup{width=.9\linewidth}
      \caption{After active timing margin customization, core frequency can be predicted with a fitted linear model, following \Eq{eq:freq-pred}.}
\label{fig:freq-pred}
\end{figure}

Because past research has shown that speed is only modestly affected by temperature~\cite{zu2016tistate}, we base our model strictly on IR voltage drop. Subtracting the IR voltage drop, which is proportional to current and hence power consumption, we derive a linear relationship between active timing margin's dynamic frequency and the chip's total power consumption as shown in  \Eq{eq:freq-pred}. The value $b$ represents a core's static CPM setting, while $k' \cdot \overline{P}$ represents the dynamic variation, dominated by the IR voltage drop. 

\begin{equation}\label{eq:freq-pred}
\begin{split}
\overline{f} &= k \cdot \overline{V_{chip}} = k \cdot (V_{vrm} - R \cdot \overline{I})\\
&= k \cdot (V_{vrm} - R \cdot \frac{\overline{P}}{V_{vrm}})\\
&= -k' \cdot \overline{P} + b
\end{split}
\end{equation}

\Fig{fig:freq-pred} shows the linear model fitted for each core's customized CPM configuration. The measured data points align with \Eq{eq:freq-pred}'s predictions. Each additional watt degrades the frequency by about two MHz. In practice, each core stores its frequency prediction model and the model is indexed by the chip's total power consumption during job scheduling and runtime.

\subsection{Delivering Critical App's Performance}
\label{sec:process:schedule:framework}

\begin{figure}
  \centering
      \includegraphics[trim=0 0 0 0,clip,width=.6\linewidth]{graphs/process/perf-pred.pdf}
      \captionsetup{width=.9\linewidth}
      \caption{Single-thread application performance can be predicted linearly using core frequency.}
\label{fig:1t-perf-pred}
\end{figure}

Frequency directly affects application performance. \Fig{fig:1t-perf-pred} shows application performance scales linearly with frequency, with different coefficients depending on the workload's memory behavior. A memory-bound workload, such as \bench{mcf}, enjoys less performance improvement from higher frequency compared with a compute-bound workload like \bench{x264} because cache misses limits the compute throughput. We, therefore, build a performance predictor for each application, using frequency as the input. In this way, thread performance on each core can be inferred by the chip's total power, using each core's frequency predictor as the intermediate step.

On a customized active timing margin system, each application's performance depends on both the core it runs on as each core has different CPM configuration which leads to varying frequency levels, and the applications running on other neighboring cores, as all applications contribute to the chip's total power which in turn affects each core's frequency through the DC voltage drop on the shared power delivery path. For some \texttt{critical} applications that the users are interested in, it is crucial that they get mapped to the customized cores that are high performance and robust. Meanwhile, it is also crucial that the co-located \texttt{background} applications are adequately managed so that the total chip power does not exceed the level that hampers critical app's core frequency. To handle this issue, we propose a scheme to selectively throttle \texttt{background} application performance to control total chip power, and indirectly frees up frequency potential for \texttt{critical} applications.

\begin{table}[h!]
  \centering
  \resizebox{0.75\columnwidth}{!}{
  \begin{tabular}{c|c|c}
    \Xhline{1pt}

    Mem behavior      & Critical & Background \\
    \hline
    Intensive  & \makecell{resnet, vgg,\\ ferret, \\fluidanimate} & \makecell{mlp, gcc,\\facesim, lu\_cb, \\streamcluster, etc.} \\
    \hline
    Non-intensive  & \makecell{squeezenet, \\seq2seq, babi, \\bodytrack, vips} & \makecell{blackScholes, x264, \\swaptions,\\ raytrace, } \\
    \Xhline{1pt}
  \end{tabular}
    }

  \caption{Classifying critical and background applications, based on their memory subsystem interference behavior.} 
  \vspace{-0.2cm}

  \label{tab:bench-type} 
\end{table}

We use the applications in \Tab{tab:bench-type} for evaluation. The \texttt{critical} workloads are user-facing and require high performance for lower latency. They include deep learning inference (CNN, RNN, and LSTM models), object detection, real-time image processing, content similarity search, etc. The \texttt{background} workloads can tolerate lower performance and include workloads such as stock price estimation, 3D image rendering, compression, compilation, and machine learning training. For our work, we focus on the performance issue caused by the active timing margin system's shared power delivery problem and excludes performance interference from the memory subsystem which is a general issue for all multicores. Thus, we avoid co-locating two memory-intensive workloads at the same time to simplify the problem.

\begin{sidewaysfigure}
  \centering
  \includegraphics[trim=10 90 10 150,clip,width=\linewidth]{graphs/process//schedule-flow/schedule-flow.pdf}
  \caption{Managing a customized active timing margin system needs to integrate the per-app performance predictor and per-core frequency predictor, so that \texttt{critical} application performance can be satisfied by using faster cores and maintaining an estimated chip power budget.}
  \label{fig:schedule-flow}
\end{sidewaysfigure}

\Fig{fig:schedule-flow} outlines our management scheme. It takes into account the core-to-core performance and robustness variation as characterized in~\Sec{sec:process:realistic}, and the inter-core application power interference on the power delivery subsystem. First, the user selects how he/she would like to set different core's CPM. The default policy uses the chip's thread-worst CPM configuration as shown in \Tab{tab:limit-delay}, obtained through our earlier characterization.

The default thread-worst policy represents a balanced trade-off between performance and reliability. Most likely workloads can execute correctly while still providing better performance. The \texttt{critical} and \texttt{background} workloads all execute correctly under thread-worst.

For higher performance, the user selects an ``aggressive'' governor, which chooses an application's most aggressive CPM configuration that guarantees correct execution. In the current approach, this can be achieved by repetitive profiling an application's CPM limits in a tier of testing servers before shipping the application to production server clusters. For most medium and light workloads in~\Tab{tab:bench-type}, thread-normal represents the high-performance policy.

For higher robustness, the user can select a ``conservative'' governor, which only schedules \texttt{background} workloads onto robust cores picked by active timing margin characterization. The robust cores are scarce and may not provide the highest performance, but they have the highest guarantee of correct execution. The conservative policy is best for unknown applications or when application correctness is paramount.

The operating system then automatically sets each core's CPM setting according to user-selected policy. The faster cores after CPM customization are selected for running \texttt{critical} application. In parallel with CPM reconfiguration, the scheme reads user-specified QoS target for the \texttt{critical} application and infers the chip power needed to meet the performance goal using per-application performance predictor and per-core frequency predictor. To meet the QoS goal, total chip power under \texttt{critical} and co-running \texttt{background} workloads cannot exceed the calculated power budget.

The manager subtracts the estimated power of the \texttt{critical} workloads from the total chip power budget to get the power envelope available for the co-running \texttt{background} jobs. The \texttt{background} jobs can then be scheduled to the same chip under this envelope by carefully tuning their power consumption. On POWER7+ where $V_{dd}$ is shared for all cores, we adjust power consumption by changing core frequency. Depending on the power envelope, we can 1)~allow workloads to use aggressive active timing margin that has the highest frequency, 2)~set cores to different DVFS states' frequency levels or 3)~use power-gating to disable cores.

\subsection{Performance Improvement}
\label{sec:process:schedule:result}

%We show in \Fig{fig:schedule-1t-results} the performance benefits provided by our scheduler to \texttt{fg} jobs in an aggressive active timing margin system. To study the impact of co-located jobs on a multi-core chip, we restrict execution to P0 of our two-socket system. We restrict the foreground job to a single core (possibly utilizing its SMT capabilities), which is a natural fit for many applications, such as LSTM and RNN inference.
%We evaluate the performance benefit of latency critical \texttt{fg} jobs in aggressive active timing margin systems in \Fig{fig:schedule-1t-results}. We co-locate all workloads onto P0 of our two-socket system to study the impact of co-located \texttt{bg} jobs on a multi-core. To highlight frequency's impact on job performance, we restrict \texttt{fg} job to use one core, which is natural for many applications, such as LSTM and RNN inference.

We evaluate our solution(s) against the static margin and the default active timing margin. Some customers turn off active timing margin for predictability. Hence the static margin is one of the fair baselines we compare with for evaluation. The system is running the stock DVFS operating system governors that already strive to improve system efficiency. Therefore, our results include that comparison implicitly. Since active timing margin systems are still new and rare, there is little other prior work to compare against directly.

Our evaluation is carried out when all cores are scheduled to run an application. In practice, power gating idle cores off when not enough workloads are available can further free up chip power and boost the performance of target workload~\cite{zu2015adaptive}. For all our tests, die temperature is maintained under 70\C, and no side effect on-chip cooling is observed. 

\begin{sidewaysfigure}
\centering 
\subfloat[Fine-tuning core-level ATM enhances application performance compared to the default ATM, but scheduling workloads to the slow core and co-locating high-power jobs limit the performance benefits.]
{
  \includegraphics[trim=0 0 0 0,clip,width=\linewidth]{graphs/process/schedule-1t-1.pdf}
  \label{fig:schedule-1t-results-1}
}

\subfloat[In a fine-tuned ATM multicore, scheduling workloads to the fast core, and throttling co-located workload power can boost performance by 15.4\%. With proper management, a 10\% performance improvement goal is guaranteed for \texttt{critical workloads} while \texttt{background workloads} performance is maximized.] 
{
  \includegraphics[trim=0 0 0 0,clip,width=\linewidth]{graphs/process/schedule-1t-2.pdf}  
  \label{fig:schedule-1t-results-2}
}
\caption{\texttt{Critical} application performance under different timing margin and scheduling settings; shown as $<\!critical:background\!>$ pairs. The result is normalized to the performance under static margin's fixed 4.2~GHz point, and the goal is to achieve 10\% improvement over static margin.}

\label{fig:schedule-1t-results}
\end{sidewaysfigure}

\Fig{fig:schedule-1t-results} summarizes the performance benefit of managing an aggressively customized active timing margin system. To highlight frequency interference's impact, we use one core to run \texttt{critical} application, which is a natural fit for many applications, such as LSTM and RNN inference. We co-locate all \texttt{critical} and \texttt{background} applications on processor 0 (P0) of our two-socket server.

Under static margin, the default DVFS governor makes POWER7+ processors clock at fixed 4.2~GHz to run applications, providing predictable but low performance. 

In~\Fig{fig:schedule-1t-results-1}, the default active timing margin improves performance uniformly for all cores, not with the highest efficiency. An unmanaged system ignores the sensitivity of core frequency to total chip power. active timing margin may be indiscriminately activated on all cores, both for \texttt{critical} and \texttt{background} workloads, which significantly raises total chip power, eroding timing margin and reducing all cores' frequency, thereby diminishing the \texttt{critical} application performance. This unmanaged system still increases frequency thanks to active timing margin's harnessed margin, but the improvement is restricted to only 6.1\% on average.

Aggressively customized active timing margin provides more frequency gain, but an unmanaged system prevents the processor from providing maximum benefit. Compared with the default active timing margin, an unmanaged processor system may carelessly assign the slowest core after CPM reconfiguration to a \texttt{critical} job, limiting the peak performance that can be achieved. The unmanaged system may also let all co-located \texttt{background} workloads run under their highest frequency, increasing total chip power and reducing \texttt{critical} workload frequency. However, in this scenario \texttt{critical} applications still enjoy 10.2\% improvement over static margin because customizing active timing margin unlocks substantial frequency gain.

In~\Fig{fig:schedule-1t-results-2}, a managed active timing margin system can opt to maximize the performance of \texttt{critical} applications. Specifically, \texttt{critical} applications get assigned to the fastest cores, and \texttt{background} application power is minimized by applying the lowest p-state. In this way, \texttt{critical} application frequency is maximized, at the cost of \texttt{background} workload performance. On average, \texttt{critical} workload performance improves by 15.2\%.

Alternatively, a managed active timing margin system can opt to balance \texttt{critical} and \texttt{background} jobs by letting \texttt{critical} applications just meet their performance goal, and maximizing \texttt{background} performance under that promise. Suppose the user targets 10\% performance improvement for a \texttt{critical} workload over the static margin run, our managed system then throttles \texttt{background} core frequencies with the minimal amount to control total chip power, letting the frequency of the core running \texttt{critical} workload reach the level that delivers target performance. Compared with the schedule that maximizes \texttt{critical} application performance, the managed schedule on average doubles the frequency of cores running \texttt{background} workloads, which is estimated to provide over 50\% performance \texttt{background} application performance improvement.

In \Fig{fig:schedule-1t-results} the performance of \bench{squeezenet}, \bench{ferret}, \bench{vgg19}, and \bench{fluidanimate} exceeds the 10\% improvment target when the chip aims at maximizing their performance. However, their performance drops below the target when the chip puts all cores into customized active timing margin states. A balanced point can be obtained by controlling \texttt{background} workload frequency. In this case, the frequency of co-located \bench{lu\_cb}, \bench{raytrace}, \bench{swaptions}, and \bench{x264} is set to the 4.2~GHz p-state.

On the contrary, \bench{seq2seq} outperform the 10\% improvement goal when its co-located \bench{streamcluster} runs under customized active timing margin. This is because \bench{streamcluster} consumes little power even when the frequency is high. The extra available power budget can be exploited by swapping \bench{streamcluster} with a more power-hungry co-runner, \bench{lu\_cb}, with core frequency properly throttled. 

The other \texttt{critical} and \texttt{background} workloads combinations meet the QoS target when active timing margin is aggressively customized for all cores. The high-frequency gain of active timing margin customization provides this benefit. For these cases, no core throttling needs to take place.

In summary, core-level active timing margin customization and active timing margin-aware application power management provide 5\% to 10\% steady performance improvement over the original active timing margin system. This result is notable because the improvement comes on a production-grade system where even a 1\% performance gain is considered significant.

\section{Related Work}
\label{sec:process:related}

There is a plethora of work on process variation, inter-core performance heterogeneity, and multicore scheduling~\cite{liang2007process,sarangi2008varius,teodorescu2008variation,rangan2009thread,dighe2010within,rangan2011achieving}. We leverage active timing margin's capability of tracking a core's inherent speed and do not need prior knowledge on the core's max frequency. Our proposal for core-level active timing margin customization conveniently expose the inter-core performance heterogeneity and help users leverage it.

Prior art has shown multi-core performance interference through the memory subsystem~\cite{mars11micro,tang11isca,delimitrou2014quasar,lo2015heracles,verma2015large,llull2017cooper}. Our work is the first to show that on an active timing margin system, the shared power delivery subsystem introduces a new dimension of resource contention, and proper management is required to reap active timing margin's full performance benefits in a predictable way.

